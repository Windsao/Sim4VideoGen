diff --git a/train_wan_with_motion.py b/train_wan_with_motion.py
index ece342b..4bf0b49 100644
--- a/train_wan_with_motion.py
+++ b/train_wan_with_motion.py
@@ -53,6 +53,7 @@ from diffsynth.models.wan_video_dit_motion import (
     MotionVectorHead, DepthHead,
     compute_motion_loss, compute_depth_loss, compute_warp_loss
 )
+from diffsynth.models.rgb_warp_loss import compute_rgb_warp_loss
 from diffsynth.models.spatiotemporal_depth_head import (
     SpatioTemporalDepthHead, SpatioTemporalDepthHeadSimple
 )
@@ -113,6 +114,12 @@ class MotionAwareWanTrainingModule(DiffusionTrainingModule):
         use_warp_loss: bool = False,
         warp_loss_weight: float = 0.1,
         warp_loss_type: str = "mse",
+        # RGB warp loss parameters (photometric temporal consistency)
+        use_rgb_warp_loss: bool = False,
+        rgb_warp_loss_weight: float = 0.1,
+        rgb_warp_loss_type: str = "l1",
+        rgb_warp_use_ssim: bool = False,
+        rgb_warp_ssim_weight: float = 0.85,
         # Spatio-temporal depth head parameters
         use_spatiotemporal_depth: bool = False,
         spatiotemporal_depth_type: str = "simple",  # "simple" or "full"
@@ -161,6 +168,13 @@ class MotionAwareWanTrainingModule(DiffusionTrainingModule):
         self.warp_loss_weight = warp_loss_weight if self.use_warp_loss else 0.0
         self.warp_loss_type = warp_loss_type
 
+        # RGB warp loss parameters (requires motion flow GT)
+        self.use_rgb_warp_loss = use_rgb_warp_loss
+        self.rgb_warp_loss_weight = rgb_warp_loss_weight if use_rgb_warp_loss else 0.0
+        self.rgb_warp_loss_type = rgb_warp_loss_type
+        self.rgb_warp_use_ssim = rgb_warp_use_ssim
+        self.rgb_warp_ssim_weight = rgb_warp_ssim_weight
+
         # Spatio-temporal depth parameters
         self.use_spatiotemporal_depth = use_spatiotemporal_depth
         self.spatiotemporal_depth_type = spatiotemporal_depth_type
@@ -424,6 +438,10 @@ class MotionAwareWanTrainingModule(DiffusionTrainingModule):
         if depth_maps is not None:
             inputs_shared["target_depth_maps"] = depth_maps
 
+        # Store raw video frames for RGB warp loss (if enabled)
+        if self.use_rgb_warp_loss and video_frames is not None:
+            inputs_shared["target_rgb_frames"] = video_frames
+
         # Extra inputs
         for extra_input in self.extra_inputs:
             if extra_input == "input_image":
@@ -527,9 +545,10 @@ class MotionAwareWanTrainingModule(DiffusionTrainingModule):
         if inputs is None:
             inputs = self.forward_preprocess(data)
 
-        # Get target motion vectors and depth maps if available
+        # Get target motion vectors, depth maps, and RGB frames if available
         target_motion = inputs.pop("target_motion_vectors", None)
         target_depth = inputs.pop("target_depth_maps", None)
+        target_rgb_frames = inputs.pop("target_rgb_frames", None)
 
         # Standard training loss computation
         max_timestep_boundary = int(inputs.get("max_timestep_boundary", 1) * self.pipe.scheduler.num_train_timesteps)
@@ -830,6 +849,65 @@ class MotionAwareWanTrainingModule(DiffusionTrainingModule):
             else:
                 total_loss = total_loss + weighted_warp_loss
 
+        # Compute RGB warp loss if enabled (photometric temporal consistency)
+        rgb_warp_loss = None
+        if self.use_rgb_warp_loss and target_rgb_frames is not None and target_motion is not None:
+            # Convert PIL images to tensor (B, 3, T, H, W)
+            import torchvision.transforms.functional as TF
+
+            # Stack frames into tensor
+            rgb_tensors = []
+            for frame in target_rgb_frames:
+                rgb_tensor = TF.to_tensor(frame)  # (3, H, W), values in [0, 1]
+                rgb_tensors.append(rgb_tensor)
+            target_rgb_tensor = torch.stack(rgb_tensors, dim=1).unsqueeze(0)  # (1, 3, T, H, W)
+            target_rgb_tensor = target_rgb_tensor.to(device=self.pipe.device, dtype=self.pipe.torch_dtype)
+
+            # Resize motion flow to match RGB size if needed
+            B_rgb, C_rgb, T_rgb, H_rgb, W_rgb = target_rgb_tensor.shape
+            target_motion_for_rgb = target_motion.to(device=self.pipe.device, dtype=self.pipe.torch_dtype)
+
+            # Motion flow shape: (B, C, T-1, H_m, W_m)
+            B_m, C_m, T_m, H_m, W_m = target_motion_for_rgb.shape
+
+            if H_m != H_rgb or W_m != W_rgb:
+                # Resize motion flow to match RGB resolution
+                # Reshape to (B * T_m, C, H, W) for interpolation
+                motion_reshaped = target_motion_for_rgb.permute(0, 2, 1, 3, 4).reshape(B_m * T_m, C_m, H_m, W_m)
+                motion_resized = F.interpolate(motion_reshaped, size=(H_rgb, W_rgb), mode='bilinear', align_corners=True)
+                target_motion_for_rgb = motion_resized.reshape(B_m, T_m, C_m, H_rgb, W_rgb).permute(0, 2, 1, 3, 4)
+
+                # Scale flow values proportionally to resolution change
+                scale_h = H_rgb / H_m
+                scale_w = W_rgb / W_m
+                target_motion_for_rgb[:, 0] *= scale_w  # dx
+                target_motion_for_rgb[:, 1] *= scale_h  # dy
+
+            # For RGB warp loss, we use the target RGB itself (self-consistency)
+            # Warp frame t to t+1 and compare with actual frame t+1
+            rgb_warp_loss = compute_rgb_warp_loss(
+                target_rgb_tensor,  # Use target as both pred and target for self-consistency
+                target_rgb_tensor,
+                target_motion_for_rgb,
+                loss_type=self.rgb_warp_loss_type,
+                use_ssim=self.rgb_warp_use_ssim,
+                ssim_weight=self.rgb_warp_ssim_weight,
+            )
+
+            # NaN check
+            if torch.isnan(rgb_warp_loss) or torch.isinf(rgb_warp_loss):
+                print(f"[WARNING] NaN/Inf rgb_warp_loss detected, skipping...")
+                rgb_warp_loss = torch.tensor(0.0, device=self.pipe.device, dtype=self.pipe.torch_dtype, requires_grad=True)
+
+            loss_dict["rgb_warp_loss"] = rgb_warp_loss.detach().item()
+            loss_dict["rgb_warp_loss_weighted"] = (self.rgb_warp_loss_weight * rgb_warp_loss).detach().item()
+
+            weighted_rgb_warp_loss = self.rgb_warp_loss_weight * rgb_warp_loss
+            if total_loss is None:
+                total_loss = weighted_rgb_warp_loss
+            else:
+                total_loss = total_loss + weighted_rgb_warp_loss
+
         # Safety check: ensure we have some loss to backpropagate
         if total_loss is None:
             # This happens when target data is missing (e.g., no depth maps for depth_only mode)
@@ -1106,6 +1184,37 @@ def motion_aware_wan_parser():
         help="Type of warp loss (default: mse)",
     )
 
+    # RGB warp loss arguments (photometric temporal consistency)
+    parser.add_argument(
+        "--use_rgb_warp_loss",
+        action="store_true",
+        help="Enable RGB warp loss for photometric temporal consistency (requires motion flow GT)",
+    )
+    parser.add_argument(
+        "--rgb_warp_loss_weight",
+        type=float,
+        default=0.1,
+        help="Weight for RGB warp loss (default: 0.1)",
+    )
+    parser.add_argument(
+        "--rgb_warp_loss_type",
+        type=str,
+        default="l1",
+        choices=["mse", "l1", "smooth_l1", "charbonnier"],
+        help="Type of RGB warp loss (default: l1)",
+    )
+    parser.add_argument(
+        "--rgb_warp_use_ssim",
+        action="store_true",
+        help="Combine RGB warp loss with SSIM loss for better structure preservation",
+    )
+    parser.add_argument(
+        "--rgb_warp_ssim_weight",
+        type=float,
+        default=0.85,
+        help="Weight for SSIM in combined loss (default: 0.85, pixel loss weight = 1 - ssim_weight)",
+    )
+
     # Spatio-temporal depth head arguments
     parser.add_argument(
         "--use_spatiotemporal_depth",
@@ -1222,6 +1331,11 @@ def launch_training_task_with_wandb(
             "use_warp_loss": args.use_warp_loss,
             "warp_loss_weight": args.warp_loss_weight,
             "warp_loss_type": args.warp_loss_type,
+            "use_rgb_warp_loss": args.use_rgb_warp_loss,
+            "rgb_warp_loss_weight": args.rgb_warp_loss_weight,
+            "rgb_warp_loss_type": args.rgb_warp_loss_type,
+            "rgb_warp_use_ssim": args.rgb_warp_use_ssim,
+            "rgb_warp_ssim_weight": args.rgb_warp_ssim_weight,
             "motion_channels": args.motion_channels,
             "lora_rank": args.lora_rank if hasattr(args, 'lora_rank') else None,
             "height": args.height,
@@ -1274,6 +1388,7 @@ def launch_training_task_with_wandb(
             "motion_loss": [],
             "depth_loss": [],
             "warp_loss": [],
+            "rgb_warp_loss": [],
             "total_loss": [],
         }
 
@@ -1350,6 +1465,8 @@ def launch_training_task_with_wandb(
                         epoch_losses["depth_loss"].append(loss_dict["depth_loss"])
                     if "warp_loss" in loss_dict:
                         epoch_losses["warp_loss"].append(loss_dict["warp_loss"])
+                    if "rgb_warp_loss" in loss_dict:
+                        epoch_losses["rgb_warp_loss"].append(loss_dict["rgb_warp_loss"])
 
                 # Update progress bar
                 status = "SKIP" if sample_skipped else f"{loss_dict['total_loss']:.4f}"
@@ -1361,6 +1478,8 @@ def launch_training_task_with_wandb(
                 }
                 if "warp_loss" in loss_dict:
                     postfix_dict["warp"] = f"{loss_dict['warp_loss']:.4f}"
+                if "rgb_warp_loss" in loss_dict:
+                    postfix_dict["rgb_warp"] = f"{loss_dict['rgb_warp_loss']:.4f}"
                 progress_bar.set_postfix(postfix_dict)
 
                 # Log to wandb (skip for skipped samples)
@@ -1387,6 +1506,10 @@ def launch_training_task_with_wandb(
                         log_data["train/warp_loss"] = loss_dict["warp_loss"]
                         log_data["train/warp_loss_weighted"] = loss_dict.get("warp_loss_weighted", loss_dict["warp_loss"])
 
+                    if "rgb_warp_loss" in loss_dict:
+                        log_data["train/rgb_warp_loss"] = loss_dict["rgb_warp_loss"]
+                        log_data["train/rgb_warp_loss_weighted"] = loss_dict.get("rgb_warp_loss_weighted", loss_dict["rgb_warp_loss"])
+
                     wandb.log(log_data, step=global_step)
 
                     # Debug output (first few logs)
@@ -1420,6 +1543,9 @@ def launch_training_task_with_wandb(
             if epoch_losses["warp_loss"]:
                 epoch_summary["epoch/warp_loss_avg"] = sum(epoch_losses["warp_loss"]) / len(epoch_losses["warp_loss"])
 
+            if epoch_losses["rgb_warp_loss"]:
+                epoch_summary["epoch/rgb_warp_loss_avg"] = sum(epoch_losses["rgb_warp_loss"]) / len(epoch_losses["rgb_warp_loss"])
+
             wandb.log(epoch_summary, step=global_step)
 
         # Save epoch checkpoint
@@ -1456,6 +1582,13 @@ if __name__ == "__main__":
     if args.use_warp_loss:
         print(f"  Weight: {args.warp_loss_weight}")
         print(f"  Type: {args.warp_loss_type}")
+    print(f"RGB warp loss enabled: {args.use_rgb_warp_loss}")
+    if args.use_rgb_warp_loss:
+        print(f"  Weight: {args.rgb_warp_loss_weight}")
+        print(f"  Type: {args.rgb_warp_loss_type}")
+        print(f"  Use SSIM: {args.rgb_warp_use_ssim}")
+        if args.rgb_warp_use_ssim:
+            print(f"  SSIM weight: {args.rgb_warp_ssim_weight}")
     if args.use_spatiotemporal_depth:
         print(f"  Type: {args.spatiotemporal_depth_type}")
         print(f"  Temporal heads: {args.num_temporal_heads}")
@@ -1520,6 +1653,12 @@ if __name__ == "__main__":
         use_warp_loss=args.use_warp_loss,
         warp_loss_weight=args.warp_loss_weight,
         warp_loss_type=args.warp_loss_type,
+        # RGB warp loss parameters
+        use_rgb_warp_loss=args.use_rgb_warp_loss,
+        rgb_warp_loss_weight=args.rgb_warp_loss_weight,
+        rgb_warp_loss_type=args.rgb_warp_loss_type,
+        rgb_warp_use_ssim=args.rgb_warp_use_ssim,
+        rgb_warp_ssim_weight=args.rgb_warp_ssim_weight,
         # Spatio-temporal depth head parameters
         use_spatiotemporal_depth=args.use_spatiotemporal_depth,
         spatiotemporal_depth_type=args.spatiotemporal_depth_type,
diff --git a/wandb/debug-internal.log b/wandb/debug-internal.log
index 995343b..a601128 120000
--- a/wandb/debug-internal.log
+++ b/wandb/debug-internal.log
@@ -1 +1 @@
-run-20251216_022505-cqgxy7mm/logs/debug-internal.log
\ No newline at end of file
+run-20251218_103116-a06sme8s/logs/debug-internal.log
\ No newline at end of file
diff --git a/wandb/debug.log b/wandb/debug.log
index c867041..a73f52b 120000
--- a/wandb/debug.log
+++ b/wandb/debug.log
@@ -1 +1 @@
-run-20251216_022505-cqgxy7mm/logs/debug.log
\ No newline at end of file
+run-20251218_103116-a06sme8s/logs/debug.log
\ No newline at end of file
diff --git a/wandb/latest-run b/wandb/latest-run
index e8162b1..2e28ea0 120000
--- a/wandb/latest-run
+++ b/wandb/latest-run
@@ -1 +1 @@
-run-20251216_022505-cqgxy7mm
\ No newline at end of file
+run-20251218_103116-a06sme8s
\ No newline at end of file
diff --git a/wandb/run-20251216_022505-cqgxy7mm/files/output.log b/wandb/run-20251216_022505-cqgxy7mm/files/output.log
index 807690a..5f73e57 100644
--- a/wandb/run-20251216_022505-cqgxy7mm/files/output.log
+++ b/wandb/run-20251216_022505-cqgxy7mm/files/output.log
@@ -33,10 +33,40 @@ Epoch 2/5: 100%|█████████████████████
 [INFO] Saved depth head to: /nyx-storage1/hanliu/world_model_ckpt/Wan-AI/sim_physics_Wan2.1_lora_warp_loss_head_full/depth_head_step-900.safetensors
 [INFO] Saved motion head to: /nyx-storage1/hanliu/world_model_ckpt/Wan-AI/sim_physics_Wan2.1_lora_warp_loss_head_full/motion_head_step-1000.safetensors
 [INFO] Saved depth head to: /nyx-storage1/hanliu/world_model_ckpt/Wan-AI/sim_physics_Wan2.1_lora_warp_loss_head_full/depth_head_step-1000.safetensors
-Epoch 3/5:  57%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                                             | 297/525 [1:45:28<1:18:04, 20.55s/it, loss=0.3195, noise=0.0586, motion=1.4141, depth=0.2812, warp=0.2832]
+Epoch 3/5: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 525/525 [3:06:22<00:00, 21.30s/it, loss=0.1885, noise=0.0575, motion=0.5938, depth=0.0530, warp=0.0535]
 [INFO] Saved motion head to: /nyx-storage1/hanliu/world_model_ckpt/Wan-AI/sim_physics_Wan2.1_lora_warp_loss_head_full/motion_head_step-1100.safetensors
 [INFO] Saved depth head to: /nyx-storage1/hanliu/world_model_ckpt/Wan-AI/sim_physics_Wan2.1_lora_warp_loss_head_full/depth_head_step-1100.safetensors
 [INFO] Saved motion head to: /nyx-storage1/hanliu/world_model_ckpt/Wan-AI/sim_physics_Wan2.1_lora_warp_loss_head_full/motion_head_step-1200.safetensors
 [INFO] Saved depth head to: /nyx-storage1/hanliu/world_model_ckpt/Wan-AI/sim_physics_Wan2.1_lora_warp_loss_head_full/depth_head_step-1200.safetensors
 [INFO] Saved motion head to: /nyx-storage1/hanliu/world_model_ckpt/Wan-AI/sim_physics_Wan2.1_lora_warp_loss_head_full/motion_head_step-1300.safetensors
 [INFO] Saved depth head to: /nyx-storage1/hanliu/world_model_ckpt/Wan-AI/sim_physics_Wan2.1_lora_warp_loss_head_full/depth_head_step-1300.safetensors
+[INFO] Saved motion head to: /nyx-storage1/hanliu/world_model_ckpt/Wan-AI/sim_physics_Wan2.1_lora_warp_loss_head_full/motion_head_step-1400.safetensors
+[INFO] Saved depth head to: /nyx-storage1/hanliu/world_model_ckpt/Wan-AI/sim_physics_Wan2.1_lora_warp_loss_head_full/depth_head_step-1400.safetensors
+[INFO] Saved motion head to: /nyx-storage1/hanliu/world_model_ckpt/Wan-AI/sim_physics_Wan2.1_lora_warp_loss_head_full/motion_head_step-1500.safetensors
+[INFO] Saved depth head to: /nyx-storage1/hanliu/world_model_ckpt/Wan-AI/sim_physics_Wan2.1_lora_warp_loss_head_full/depth_head_step-1500.safetensors
+Epoch 4/5: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 525/525 [3:07:00<00:00, 21.37s/it, loss=0.1487, noise=0.0592, motion=0.1973, depth=0.0339, warp=0.0344]
+[INFO] Saved motion head to: /nyx-storage1/hanliu/world_model_ckpt/Wan-AI/sim_physics_Wan2.1_lora_warp_loss_head_full/motion_head_step-1600.safetensors
+[INFO] Saved depth head to: /nyx-storage1/hanliu/world_model_ckpt/Wan-AI/sim_physics_Wan2.1_lora_warp_loss_head_full/depth_head_step-1600.safetensors
+[INFO] Saved motion head to: /nyx-storage1/hanliu/world_model_ckpt/Wan-AI/sim_physics_Wan2.1_lora_warp_loss_head_full/motion_head_step-1700.safetensors
+[INFO] Saved depth head to: /nyx-storage1/hanliu/world_model_ckpt/Wan-AI/sim_physics_Wan2.1_lora_warp_loss_head_full/depth_head_step-1700.safetensors
+[INFO] Saved motion head to: /nyx-storage1/hanliu/world_model_ckpt/Wan-AI/sim_physics_Wan2.1_lora_warp_loss_head_full/motion_head_step-1800.safetensors
+[INFO] Saved depth head to: /nyx-storage1/hanliu/world_model_ckpt/Wan-AI/sim_physics_Wan2.1_lora_warp_loss_head_full/depth_head_step-1800.safetensors
+[INFO] Saved motion head to: /nyx-storage1/hanliu/world_model_ckpt/Wan-AI/sim_physics_Wan2.1_lora_warp_loss_head_full/motion_head_step-1900.safetensors
+[INFO] Saved depth head to: /nyx-storage1/hanliu/world_model_ckpt/Wan-AI/sim_physics_Wan2.1_lora_warp_loss_head_full/depth_head_step-1900.safetensors
+[INFO] Saved motion head to: /nyx-storage1/hanliu/world_model_ckpt/Wan-AI/sim_physics_Wan2.1_lora_warp_loss_head_full/motion_head_step-2000.safetensors
+[INFO] Saved depth head to: /nyx-storage1/hanliu/world_model_ckpt/Wan-AI/sim_physics_Wan2.1_lora_warp_loss_head_full/depth_head_step-2000.safetensors
+[INFO] Saved motion head to: /nyx-storage1/hanliu/world_model_ckpt/Wan-AI/sim_physics_Wan2.1_lora_warp_loss_head_full/motion_head_step-2100.safetensors
+[INFO] Saved depth head to: /nyx-storage1/hanliu/world_model_ckpt/Wan-AI/sim_physics_Wan2.1_lora_warp_loss_head_full/depth_head_step-2100.safetensors
+Epoch 5/5: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 525/525 [3:06:03<00:00, 21.26s/it, loss=0.1698, noise=0.0709, motion=0.2451, depth=0.0215, warp=0.0217]
+[INFO] Saved motion head to: /nyx-storage1/hanliu/world_model_ckpt/Wan-AI/sim_physics_Wan2.1_lora_warp_loss_head_full/motion_head_step-2200.safetensors
+[INFO] Saved depth head to: /nyx-storage1/hanliu/world_model_ckpt/Wan-AI/sim_physics_Wan2.1_lora_warp_loss_head_full/depth_head_step-2200.safetensors
+[INFO] Saved motion head to: /nyx-storage1/hanliu/world_model_ckpt/Wan-AI/sim_physics_Wan2.1_lora_warp_loss_head_full/motion_head_step-2300.safetensors
+[INFO] Saved depth head to: /nyx-storage1/hanliu/world_model_ckpt/Wan-AI/sim_physics_Wan2.1_lora_warp_loss_head_full/depth_head_step-2300.safetensors
+[INFO] Saved motion head to: /nyx-storage1/hanliu/world_model_ckpt/Wan-AI/sim_physics_Wan2.1_lora_warp_loss_head_full/motion_head_step-2400.safetensors
+[INFO] Saved depth head to: /nyx-storage1/hanliu/world_model_ckpt/Wan-AI/sim_physics_Wan2.1_lora_warp_loss_head_full/depth_head_step-2400.safetensors
+[INFO] Saved motion head to: /nyx-storage1/hanliu/world_model_ckpt/Wan-AI/sim_physics_Wan2.1_lora_warp_loss_head_full/motion_head_step-2500.safetensors
+[INFO] Saved depth head to: /nyx-storage1/hanliu/world_model_ckpt/Wan-AI/sim_physics_Wan2.1_lora_warp_loss_head_full/depth_head_step-2500.safetensors
+[INFO] Saved motion head to: /nyx-storage1/hanliu/world_model_ckpt/Wan-AI/sim_physics_Wan2.1_lora_warp_loss_head_full/motion_head_step-2600.safetensors
+[INFO] Saved depth head to: /nyx-storage1/hanliu/world_model_ckpt/Wan-AI/sim_physics_Wan2.1_lora_warp_loss_head_full/depth_head_step-2600.safetensors
+[INFO] Saved motion head to: /nyx-storage1/hanliu/world_model_ckpt/Wan-AI/sim_physics_Wan2.1_lora_warp_loss_head_full/motion_head_step-2625.safetensors
+[INFO] Saved depth head to: /nyx-storage1/hanliu/world_model_ckpt/Wan-AI/sim_physics_Wan2.1_lora_warp_loss_head_full/depth_head_step-2625.safetensors
diff --git a/wandb/run-20251216_022505-cqgxy7mm/logs/debug-internal.log b/wandb/run-20251216_022505-cqgxy7mm/logs/debug-internal.log
index f7a5ecb..83f7061 100644
--- a/wandb/run-20251216_022505-cqgxy7mm/logs/debug-internal.log
+++ b/wandb/run-20251216_022505-cqgxy7mm/logs/debug-internal.log
@@ -4,3 +4,10 @@
 {"time":"2025-12-16T02:25:05.67542694-06:00","level":"INFO","msg":"stream: started","id":"cqgxy7mm"}
 {"time":"2025-12-16T02:25:05.675438074-06:00","level":"INFO","msg":"writer: started","stream_id":"cqgxy7mm"}
 {"time":"2025-12-16T02:25:05.675442212-06:00","level":"INFO","msg":"sender: started","stream_id":"cqgxy7mm"}
+{"time":"2025-12-16T13:34:35.899630054-06:00","level":"INFO","msg":"api: retrying HTTP error","status":502,"url":"https://api.wandb.ai/files/magicslabnorthwestern/Sim4Videos/cqgxy7mm/file_stream","body":"\n<html><head>\n<meta http-equiv=\"content-type\" content=\"text/html;charset=utf-8\">\n<title>502 Server Error</title>\n</head>\n<body text=#000000 bgcolor=#ffffff>\n<h1>Error: Server Error</h1>\n<h2>The server encountered a temporary error and could not complete your request.<p>Please try again in 30 seconds.</h2>\n<h2></h2>\n</body></html>\n"}
+{"time":"2025-12-16T17:58:34.442788703-06:00","level":"INFO","msg":"fileTransfer: Close: file transfer manager closed"}
+{"time":"2025-12-16T17:58:34.677867841-06:00","level":"INFO","msg":"handler: operation stats","stats":{}}
+{"time":"2025-12-16T17:58:34.68097608-06:00","level":"INFO","msg":"stream: closing","id":"cqgxy7mm"}
+{"time":"2025-12-16T17:58:34.680985326-06:00","level":"INFO","msg":"handler: closed","stream_id":"cqgxy7mm"}
+{"time":"2025-12-16T17:58:34.681547437-06:00","level":"INFO","msg":"sender: closed","stream_id":"cqgxy7mm"}
+{"time":"2025-12-16T17:58:34.681559866-06:00","level":"INFO","msg":"stream: closed","id":"cqgxy7mm"}
diff --git a/wandb/run-20251216_022505-cqgxy7mm/logs/debug.log b/wandb/run-20251216_022505-cqgxy7mm/logs/debug.log
index 9b9557c..e0ef0b0 100644
--- a/wandb/run-20251216_022505-cqgxy7mm/logs/debug.log
+++ b/wandb/run-20251216_022505-cqgxy7mm/logs/debug.log
@@ -19,3 +19,8 @@ config: {'learning_rate': 0.0001, 'weight_decay': 0.01, 'num_epochs': 5, 'gradie
 2025-12-16 02:25:06,095 INFO    MainThread:3988311 [wandb_run.py:_redirect():2438] Wrapping output streams.
 2025-12-16 02:25:06,095 INFO    MainThread:3988311 [wandb_run.py:_redirect():2461] Redirects installed.
 2025-12-16 02:25:06,099 INFO    MainThread:3988311 [wandb_init.py:init():1081] run started, returning control to user process
+2025-12-16 17:58:34,058 INFO    MainThread:3988311 [wandb_run.py:_finish():2287] finishing run magicslabnorthwestern/Sim4Videos/cqgxy7mm
+2025-12-16 17:58:34,059 INFO    MainThread:3988311 [wandb_run.py:_atexit_cleanup():2486] got exitcode: 0
+2025-12-16 17:58:34,059 INFO    MainThread:3988311 [wandb_run.py:_restore():2468] restore
+2025-12-16 17:58:34,059 INFO    MainThread:3988311 [wandb_run.py:_restore():2474] restore done
+2025-12-16 17:58:34,680 INFO    MainThread:3988311 [wandb_run.py:_footer_sync_info():3862] logging synced files
diff --git a/wandb/run-20251216_022505-cqgxy7mm/run-cqgxy7mm.wandb b/wandb/run-20251216_022505-cqgxy7mm/run-cqgxy7mm.wandb
index b27b802..03cc59c 100644
Binary files a/wandb/run-20251216_022505-cqgxy7mm/run-cqgxy7mm.wandb and b/wandb/run-20251216_022505-cqgxy7mm/run-cqgxy7mm.wandb differ
